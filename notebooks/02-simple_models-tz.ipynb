{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Evaluate Simple Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "import joblib\n",
    "import os\n",
    "import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from tqdm import tqdm\n",
    "# from typing import List, Set\n",
    "import numpy as np\n",
    "\n",
    "# from sklearn.impute import SimpleImputer\n",
    "# from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_curve, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 17\n",
    "use_topics = True\n",
    "project_base = os.path.dirname(os.path.realpath('.'))\n",
    "print(f'Project base path: {project_base}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function\n",
    "def make_kaggle_preds(fitted_model, data, output_file_name):\n",
    "    preds = fitted_model.predict(data)\n",
    "    kaggle_preds_df = pd.DataFrame({'id': list(range(len(preds))), 'label':preds})\n",
    "    kaggle_preds_df['label'] = kaggle_preds_df['label'].astype(int)\n",
    "    kaggle_preds_df.to_csv(os.path.join(project_base, 'data', 'kaggle_preds', output_file_name), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_topics:\n",
    "    X_train_processed = pd.read_csv(os.path.join(project_base, 'data', 'cleaned_data', 'processed_train_data_w_topics.csv'))\n",
    "    X_test_processed = pd.read_csv(os.path.join(project_base, 'data', 'cleaned_data', 'processed_test_data_w_topics.csv'))\n",
    "else:\n",
    "    X_train_processed = pd.read_csv(os.path.join(project_base, 'data', 'cleaned_data', 'processed_train_data.csv'))\n",
    "    X_test_processed = pd.read_csv(os.path.join(project_base, 'data', 'cleaned_data', 'processed_test_data.csv'))\n",
    "X_kaggle_processed = pd.read_csv(os.path.join(project_base, 'data', 'cleaned_data', 'processed_kaggle_data.csv'))\n",
    "\n",
    "\n",
    "y_train = pd.read_csv(os.path.join(project_base,  'data', 'cleaned_data', 'processed_train_y.csv'))\n",
    "y_test = pd.read_csv(os.path.join(project_base, 'data', 'cleaned_data', 'processed_test_y.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f' Train set shape: {X_train_processed.shape}')\n",
    "print(f' Test set shape: {X_test_processed.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "dummy_classifier = DummyClassifier(strategy='uniform', random_state=RANDOM_SEED)\n",
    "\n",
    "dummy_classifier.fit(X_train_processed, y_train)\n",
    "dummy_classifier.score(X_test_processed, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kaggle predictions\n",
    "make_kaggle_preds(fitted_model=dummy_classifier, data=X_kaggle_processed, output_file_name='dummy_preds.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd_classifier = SGDClassifier()\n",
    "\n",
    "sgd_classifier.fit(X_train_processed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_test_preds = sgd_classifier.predict(X_test_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Accuracy Score:\\n\\n{accuracy_score(y_test, sgd_test_preds)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Classification Report:\\n\\n{classification_report(y_test, sgd_test_preds)}')\n",
    "print(f'Confusion Matrix:\\n\\n{confusion_matrix(y_test, sgd_test_preds)}')\n",
    "p, r, threshold = precision_recall_curve(y_test, sgd_test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_kaggle_preds(fitted_model=sgd_classifier, data=X_kaggle_processed, output_file_name='sgd_preds.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_classifier = RandomForestClassifier(random_state=RANDOM_SEED)\n",
    "\n",
    "rf_classifier.fit(X_train_processed, y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save fitted model\n",
    "joblib.dump(rf_classifier, os.path.join(project_base, 'data', 'trained_models','rf_clf.pkl')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_test_preds = rf_classifier.predict(X_test_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Accuracy Score:\\n\\n{accuracy_score(y_test, rf_test_preds)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_kaggle_preds(fitted_model=rf_classifier, data=X_kaggle_processed, output_file_name='rf_clf_topics_preds.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest With Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [80, 90, 100, 110],\n",
    "    'max_features': [2, 3],\n",
    "    'min_samples_leaf': [3, 4, 5],\n",
    "    'min_samples_split': [8, 10, 12],\n",
    "    'n_estimators': [100, 200, 300, 1000]\n",
    "}\n",
    "\n",
    "gs_rf_clf = RandomForestClassifier()\n",
    "grid_search = GridSearchCV(estimator = gs_rf_clf, param_grid = param_grid, cv = 3, n_jobs = -1, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run grid search to find best parameters\n",
    "grid_search.fit(X_train_processed, y_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best params\n",
    "# {'bootstrap': True,\n",
    "#  'max_depth': 90,\n",
    "#  'max_features': 3,\n",
    "#  'min_samples_leaf': 3,\n",
    "#  'min_samples_split': 12,\n",
    "#  'n_estimators': 1000}\n",
    "\n",
    "# best model\n",
    "cv_rf_classifier = RandomForestClassifier(max_depth=90, max_features=3, min_samples_leaf=3,\n",
    "                       min_samples_split=12, n_estimators=1000)\n",
    "\n",
    "cv_rf_classifier.fit(X_train_processed, y_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_test_preds = cv_rf_classifier.predict(X_test_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Accuracy Score:\\n\\n{accuracy_score(y_test, rf_test_preds)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_kaggle_preds(fitted_model=cv_rf_classifier, data=X_kaggle_processed, output_file_name='cv_rf_clf_preds.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'Accuracy Score:\\n\\n{accuracy_score(y_test, rf_test_preds)}')\n",
    "# print(f'Classification Report:\\n\\n{classification_report(y_test, rf_test_preds)}')\n",
    "# print(f'Confusion Matrix:\\n\\n{confusion_matrix(y_test, rf_test_preds)}')\n",
    "# p, r, threshold = precision_recall_curve(y_test, rf_test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wc_analysis_df = pd.DataFrame({'word_count': list(X_test['word_count']), \n",
    "#                                 'true_label':y_test, \n",
    "#                                 'predicted_label': rf_test_preds})\n",
    "# print(f'Accuracy Score check: {len(wc_analysis_df[wc_analysis_df.true_label == wc_analysis_df.predicted_label]) / len(wc_analysis_df)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wc_set = set(wc_analysis_df.word_count)\n",
    "# accuracy_list = []\n",
    "# word_freq_list = []\n",
    "\n",
    "# for wc in wc_set:\n",
    "#     current_df = wc_analysis_df[wc_analysis_df.word_count == wc].copy()\n",
    "#     current_df_len = len(current_df)\n",
    "#     accuracy = len(current_df[current_df.true_label == current_df.predicted_label]) / current_df_len\n",
    "#     print(f'For samples with length {wc} the model accuracy was {accuracy*100:.3f}% with {current_df_len} total words\\n')\n",
    "#     accuracy_list.append(accuracy)\n",
    "#     word_freq_list.append(current_df_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_wc_analysis = pd.DataFrame({'word_count': list(range(1, len(accuracy_list)+1)), \n",
    "#                                  'acurracy':accuracy_list,\n",
    "#                                  'word_freq':word_freq_list})\n",
    "# # full_wc_analysis.head()\n",
    "# l = alt.Chart(full_wc_analysis).mark_line(color='red').encode(\n",
    "#     x = alt.X('word_count'),\n",
    "#     y = alt.Y('acurracy'))\n",
    "\n",
    "# b = alt.Chart(full_wc_analysis).mark_bar().encode(\n",
    "#     x = alt.X('word_count'),\n",
    "#     y = alt.Y('word_freq'))\n",
    "\n",
    "# print(f'Correlation between word frequency and accuracy: {full_wc_analysis.word_freq.corr(full_wc_analysis.acurracy)*100:.2f}%')\n",
    "# (b+l).resolve_scale(y='independent')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_wc_analysis.word_freq.corr(full_wc_analysis.acurracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # where do our error come from\n",
    "# rf_test_pred_probas = rf_classifier.predict_proba(X_test_processed)\n",
    "# rf_test_pred_probas_df = pd.DataFrame({'id': list(range(len(rf_test_pred_probas))), \n",
    "#                                        'proba':rf_test_pred_probas[:,1],\n",
    "#                                        'rounded_proba': np.round(rf_test_pred_probas[:,1]),\n",
    "#                                        'label': y_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf_test_pred_probas_df_wrong = rf_test_pred_probas_df[rf_test_pred_probas_df.label !=rf_test_pred_probas_df.rounded_proba]\n",
    "# rf_test_pred_probas_df_wrong.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf_test_pred_probas_df_wrong.groupby(['label', 'rounded_proba','proba'],\n",
    "#                                      as_index=False).count().sort_values('id', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Data for Kaggle Scoreboard "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf_kaggle_preds = rf_classifier.predict(X_kaggle_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kaggle_preds_df = pd.DataFrame({'id': list(range(len(rf_kaggle_preds))), 'label':rf_kaggle_preds})\n",
    "# kaggle_preds_df['label'] = kaggle_preds_df['label'].astype(int)\n",
    "# kaggle_preds_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kaggle_preds_df.to_csv(os.path.join(project_base, 'data', 'cleaned_data', 'rf_kaggle_preds.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # in case we want to try a different decision threshold\n",
    "# rf_kaggle_pred_probas = rf_classifier.predict_proba(X_kaggle_processed)\n",
    "# kaggle_pred_probas_df = pd.DataFrame({'id': list(range(len(rf_kaggle_pred_probas))), 'label':rf_kaggle_pred_probas[:,0]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(torch.from_numpy(X_train_processed).float(), torch.from_numpy(y_train).float())\n",
    "test_dataset = TensorDataset(torch.from_numpy(X_test_processed).float(), torch.from_numpy(y_test).float())\n",
    "# kaggle_dataset = TensorDataset(torch.from_numpy(X_kaggle_processed).float(), torch.from_numpy(y_test.values).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=512, shuffle=True, drop_last=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=512, shuffle=False, drop_last=False)\n",
    "# kaggle_dataloader = DataLoader(kaggle_dataset, batch_size=512, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "    def __init__(self, input_shape, output_shape):\n",
    "        super(NN, self).__init__()\n",
    "        \n",
    "        self.fc_1 = nn.Linear(input_shape, 8)\n",
    "        self.bn1 = nn.BatchNorm1d(8)\n",
    "        \n",
    "        self.fc_2 = nn.Linear(8, 16)\n",
    "        self.bn2 = nn.BatchNorm1d(16)\n",
    "        \n",
    "        self.fc_3 = nn.Linear(16, output_shape)\n",
    "        \n",
    "        # self.kaiming_1 = nn.kaiming_normal(fc_1.weights, 'leaky_relu')\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.fc_1(x)))\n",
    "        x = F.relu(self.bn2(self.fc_2(x)))\n",
    "        x = torch.sigmoid(self.fc_3(x))\n",
    "        return x\n",
    "    \n",
    "# check the model dimensions\n",
    "model = NN(31, 2)\n",
    "x = torch.randn(100, 31)\n",
    "print(model(x).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "input_size = 26\n",
    "output_size = 1\n",
    "learning_rate = 0.003\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NN(input_size, output_size)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "# if multiclass\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "adam = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not great performance, code still needs to be cleaned up\n",
    "for i in range(1, epochs+1):\n",
    "    \n",
    "    for idx, (x, y) in enumerate(train_dataloader):\n",
    "        \n",
    "        x, y = x.to(device), y.to(device)\n",
    "        model.train()\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # predictions\n",
    "        y_hat = model(x)\n",
    "        loss = criterion(y_hat.squeeze(dim=1), y)\n",
    "        loss.backward()\n",
    "        adam.step()\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        num_correct = 0\n",
    "        total = 0 \n",
    "        \n",
    "        for idx, (x, y) in enumerate(test_dataloader):\n",
    "        \n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            y_hat = model(x)\n",
    "\n",
    "#             print(f'Batch {idx}, Epoch {i}, Loss {loss.item()}')\n",
    "\n",
    "            \n",
    "#             _, predictions = y_hat.max(1)\n",
    "\n",
    "            y_hat_preds = y_hat.round().squeeze(dim=1)\n",
    "            num_correct += (y_hat_preds == y).sum()\n",
    "        \n",
    "#             correct = val_y.eq(torch.round(y_hat_preds.squeeze())).sum()\n",
    "#             num_correct += correct.item()\n",
    "            \n",
    "            total += y_hat.size(0)\n",
    "            \n",
    "        print(f'Epoch {i} test accuracy {num_correct / total}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_preds = model(torch.from_numpy(X_test_processed).float()).detach().numpy()\n",
    "kaggle_preds = model(torch.from_numpy(X_kaggle_processed).float()).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_class_preds = final_preds.round()\n",
    "final_kaggle_preds = kaggle_preds.round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Accuracy Score:\\n\\n{accuracy_score(y_test, final_class_preds)}')\n",
    "print(f'Classification Report:\\n\\n{classification_report(y_test, final_class_preds)}')\n",
    "print(f'Confusion Matrix:\\n\\n{confusion_matrix(y_test, final_class_preds)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_nn_pred_df = pd.DataFrame({'id': list(range(len(final_kaggle_preds))), 'label':final_kaggle_preds[:,0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_nn_pred_df.to_csv(os.path.join(project_base, 'data', 'cleaned_data', 'nn_kaggle_preds.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
