{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "import joblib\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import shap\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_curve, accuracy_score, roc_curve\n",
    "\n",
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_base = os.path.dirname(os.path.realpath('.'))\n",
    "RANDOM_SEED = 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_processed = pd.read_csv(os.path.join(project_base, 'data', 'cleaned_data', 'processed_train_data.csv'))\n",
    "X_test_processed = pd.read_csv(os.path.join(project_base, 'data', 'cleaned_data', 'processed_test_data.csv'))\n",
    "\n",
    "\n",
    "y_train = pd.read_csv(os.path.join(project_base,  'data', 'cleaned_data', 'processed_train_y.csv'))\n",
    "y_test = pd.read_csv(os.path.join(project_base, 'data', 'cleaned_data', 'processed_test_y.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf = joblib.load(os.path.join(project_base, 'data', 'trained_models','rf_clf.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_test_preds = rf_clf.predict(X_test_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Accuracy Score:\\n\\n{accuracy_score(y_test, rf_test_preds)}\\n')\n",
    "print(f'Classification Report:\\n\\n{classification_report(y_test, rf_test_preds)}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Confusion Matrix:\\n\\n{confusion_matrix(y_test, rf_test_preds)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RF Variable Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = rf_clf.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in rf_clf.estimators_], axis=0)\n",
    "\n",
    "forest_importances = pd.DataFrame(importances, index=list(X_test_processed), columns=['feature_importance'])\n",
    "# forest_importances['std'] = std\n",
    "# forest_importances.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_importances.iloc[:32,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_importances_slim = forest_importances.iloc[:32,:].copy()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "forest_importances_slim.plot.bar(yerr=std[:32], ax=ax)\n",
    "ax.set_title(\"Feature importances using MDI\")\n",
    "ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "fig.set_size_inches(12.5, 6.5)\n",
    "# fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bars = alt.Chart().mark_bar().encode(\n",
    "    x='index:O',\n",
    "    y=alt.Y('feature_importance', title='Feature Importance'),\n",
    ")\n",
    "\n",
    "error_bars = alt.Chart().mark_errorbar(extent='ci').encode(\n",
    "    x='index:O',\n",
    "    y='str:Q'\n",
    ")\n",
    "\n",
    "alt.layer(bars, error_bars, data=forest_importances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error by Sentence Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ablation Analysis (Accuracy With Different Feature Subsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train on all featuress except word2vec features\n",
    "\n",
    "# rf = RandomForestClassifier()\n",
    "# rf.fit(X_train_processed.iloc[:, :26], y_train.values)\n",
    "# preds = rf.predict(X_test_processed.iloc[:, :26])\n",
    "# print(f'Accuracy Score:\\n\\n{accuracy_score(y_test, preds)}\\n')\n",
    "# # 0.70089"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train on only word2vec features\n",
    "\n",
    "# rf = RandomForestClassifier()\n",
    "# rf.fit(X_train_processed.iloc[:, 26:], y_train.values)\n",
    "# preds = rf.predict(X_test_processed.iloc[:, 26:])\n",
    "# print(f'Accuracy Score:\\n\\n{accuracy_score(y_test, preds)}\\n')\n",
    "# # 0.66633"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_test_preds_probas = rf_clf.predict_proba(X_test_processed)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, rf_test_preds_probas[:, 1])\n",
    "roc_df = pd.DataFrame({'fpr':fpr,'tpr':tpr})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(roc_df).mark_line().encode(\n",
    "    x='fpr',\n",
    "    y='tpr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision Recall Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_, recall_, threshold = precision_recall_curve(y_test, np.round(rf_test_preds_probas[:, 1], 2))\n",
    "pr_df = pd.DataFrame({'precision':precision_, 'recall':recall_})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(pr_df).mark_line().encode(\n",
    "    x='recall',\n",
    "    y='precision')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHAP Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(rf_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP to Interpret the Specific Observations (Local Explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single random observation\n",
    "\n",
    "idx = 57\n",
    "observation = X_train_processed.iloc[[idx]]\n",
    "print(f\"Observation true label: {y_train.iloc[idx]}\")\n",
    "print(f\"Observation predicted label (proba): {rf_clf.predict_proba(observation)[:,1]}\")\n",
    "\n",
    "observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for idx in range(1000):\n",
    "    observation = X_train_processed.iloc[[idx]]\n",
    "    actual = y_train.iloc[idx].values[0]\n",
    "    pred = rf_clf.predict_proba(observation)[:,1][0]\n",
    "\n",
    "    if actual == 0 and pred > 0.5:\n",
    "        print(idx)\n",
    "        count += 1\n",
    "        if count > 4:\n",
    "            break\n",
    "#     print(f\"Observation true label: {y_train.iloc[idx]}\")\n",
    "#     print(f\"Observation predicted label (proba): {rf_clf.predict_proba(observation)[:,1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values = explainer.shap_values(observation.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one shape value for every variable\n",
    "shap_values[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you need to check in what unit the output is, run the following:\n",
    "explainer.model.tree_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average value of \"1\" across the entire data set\n",
    "explainer.expected_value[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_probas = rf_clf.predict_proba(processed_train_df)\n",
    "np.mean(predicted_probas[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf.predict_proba(observation)[:, 1] - explainer.expected_value[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(shap_values[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.force_plot(explainer.expected_value[1], shap_values[1], features=observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP to Interpret the Model (Global Explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observations = X_train_processed.sample(25, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "shap_values = explainer.shap_values(observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shap.force_plot(explainer.expected_value[1], shap_values[1], features=observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values[1], features=observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
